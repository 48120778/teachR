---
title: "Machine Learning with caret: classification"
output: html_document
---

```{r setup}
library(caret)
library(fastNaiveBayes)
library(readr)
library(functional)
library(ggplot2)
library(magrittr)
library(tidyverse)
```

# Classification with R

## KNN

*what is knn?*

## Data loading


```{r}
dataurl <- "https://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data"

wine <- read_csv(dataurl, col_names = F)
```



### Data Fixing

We want good column names, so we will follow the names here: [wine analysis link](http://dataaspirant.com/2017/01/09/knn-implementation-r-using-caret-package/)

```{r}
good_cols <- c("class",
	       "alcohol",
	       'malic_acid',
	       'ash',
	       'alkalinity',
	       'magnesium',
	       'total_phenols',
	       'flavanoids',
	       'nonflavonoids_phenols',
	       'proanthocyanins',
	       'color_intensity',
	       'hue',
	       'dilution',
	       'proline'
)

fix_cols <- function(df){
	colnames(df) <- good_cols
	df$class <- as.factor(df$class)
	df
}
wine <- fix_cols(wine)
glimpse(wine)
```


```{r}
set.seed(3033)
## WARNING: Danger function
split <- function(df, p = 0.75, list = FALSE, ...) {
	train_ind <- createDataPartition(df[[1]], p = p, list = list)
	cat("creating training dataset...\n")
	training <<- df[train_ind, ]
	cat("completed training dataset, creating test set\n")
	test <<- df[-train_ind, ]
	cat("done")
}

split(wine)

ggplot(data = wine, aes(x = malic_acid, fill = class)) + geom_density()
ggplot(data = wine, aes(x = alkalinity, fill = class)) + geom_density()
ggplot(data = wine, aes(x = ash, fill = class)) + geom_density()
ggplot(data = wine, aes(x = magnesium, fill = class)) + geom_density()
```


## Picking a value for k

First, we will set up our computer to process in parallel:

```{r}
library(doParallel)
numcores <- parallel::detectCores() - 1
cl <- makePSOCKcluster(numcores)
registerDoParallel(cl)
```

Now we will make a knn model using `caret::train`:

```{r}
set.seed(3333)
trainMethod <- trainControl(method = "repeatedcv",
			    number = 10,
			    repeats = 3)
# k-folds cross validation
# y ~ x
knn_fit <- train(class ~ ., 
		 data = training, 
		 method = "knn",
		 trControl = trainMethod,
		 preProcess = c("center", "scale"),
		 tuneLength = 10)

knn_fit
# k-Nearest Neighbors 
# 
# 135 samples
#  13 predictor
#   3 classes: '1', '2', '3' 
# 
# Pre-processing:
#  centered (13), scaled (13) 
# Resampling: Cross-Validated (10 fold, repeated 3 times) 
# Summary of sample sizes: 121, 122, 122, 121, 121, 121, ... 
# Resampling results across tuning parameters:
# 
#   k   Accuracy   Kappa    
#    5  0.9700549  0.9548756
#    7  0.9676740  0.9516351
#    9  0.9609280  0.9418362
#   11  0.9579426  0.9370280
#   13  0.9702686  0.9552588
#   15  0.9722527  0.9579543
#   17  0.9752442  0.9625294
#   19  0.9681013  0.9519242
#   21  0.9726496  0.9588742
#   23  0.9726496  0.9589829
# 
# Accuracy was used to
#  model using the
#  largest value.
# The final value used
#  for the model was k = 17.
```



Lets see what value for K we chose with our grid search:

```{r}
plot(knn_fit)
```


### Alternative with a known K:



```{r}


knn_fit2 <- knn3(training, training$class, k = 15, prob = FALSE)
knn_fit2
```

```{r}
test_pred <- predict(knn_fit, newdata = test)
test_pred
test_pred2 <- predict(knn_fit2, newdata = test, prob = F)
test_pred2
```

# Assessing the model

We do not have vector, numeric data, so how can we assess what we are doing?

## Confusion matrix

* accuracy
* precision
* recall
* F1 score

```{r}
confusionMatrix(test_pred, test$class)
```

```{r}

conmat <- function(predicted, expected){
	cm <- as.matrix(table(Actual = as.factor(expected$class), Predicted = predicted))
	cm
}
conmat(test_pred, test)
f1_score <- function(predicted, expected, positive.class="1") {
	cm = conmat(predicted, expected)
	precision <- diag(cm) / colSums(cm)
	recall <- diag(cm) / rowSums(cm)
	f1 <-  ifelse(precision + recall == 0, 0, 2 * precision * recall / (precision + recall))
	#Assuming that F1 is zero when it's not possible compute it
	f1[is.na(f1)] <- 0

	#Binary F1 or Multi-class macro-averaged F1
	ifelse(nlevels(expected) == 2, f1[positive.class], mean(f1))
}

accuracy <- function(predicted, expected){
	cm <- conmat(predicted, expected)
	sum(diag(cm)/length(test$class))
}
get_scores <- function(predictions, test){
	f1 <- f1_score(predictions,test)
	acc <- accuracy(predictions,test)
	scores <- c(accuracy = acc, f1 = f1)	
	scores
}
pander(get_scores(test_pred, test))
```

# Naive Bayes

## What is bayes theorem?
## why is it naive?

## Doing it

```{r}
nb_fit <- train(training, 
		training$class,
		trControl = trainMethod,
		method = "nb",
		tuneLength = 10
)
nb_fit
plot(nb_fit)
```

```{r}
nb_pred <- predict(nb_fit, newdata = test)
nb_pred
```

```{r}
confusionMatrix(nb_pred, test$class)
get_scores(nb_pred, test)
```


## Why not do it FAST

detect the distribution:
```{r}
library(fastNaiveBayes)
y <- training$class
x <- training[-1]
dist <- fastNaiveBayes.detect_distribution(x, nrows = nrow(x))
dist
```

Make a model:

```{r}
fast_nb_fit <- fastNaiveBayes.mixed(x,y)
fast_nb_fit
```

Make a prediction

```{r}
fast_pred <- predict(fast_nb_fit, newdata = test[-1])
fast_pred
```

Assess
```{r}
confusionMatrix(fast_pred, test$class)
get_scores(fast_pred, test)
```


# Make a pipeline



## Make up some new data


## Run new data through your machine learning pipeline

# Close shop


```{r}
stopCluster(cl)
```
