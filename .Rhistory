*what is knn?*
library(functional)
library(caret)
numcores <- parallel::detectCores() -1
numcores
library(doParallel)
registerDoParallel(cl)
cl <- makePSOCKcluster(numcores)
library(doParallel)
set.seed(655)
set.seed(655)
set.seed(655)
set.seed(655)
trainMethod <- trainControl(method = "repeatedCV",
number = 10,
repeats = 3)
knn_fit <- train(class ~ .,
data = training,
method = "knn",
trControl = trainMethod,
preProcess = c("center", "scale"),
tuneLength = 10)
## Data Loading
### Data Fixing
We want good column names, so we will follow the names here: [wine analysis link](http://dataaspirant.com/2017/01/09/knn-implementation-r-using-caret-package/)
## WARNING: Danger function
split <- function(df, p = 0.75, list = FALSE, ...) {
train_ind <- createDataPartition(df[[1]], p = p, list = list)
cat("creating training dataset...\n")
training <<- df[train_ind, ]
cat("completed training dataset, creating test set\n")
test <<- df[-train_ind, ]
cat("done")
}
split(wine)
dataurl <- "https://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data"
wine <- read_csv(dataurl, col_names = F)
library(caret)
library(fastNaiveBayes)
library(readr)
library(functional)
library(ggplot2)
library(magrittr)
library(caret)
dataurl <- "https://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data"
wine <- read_csv(dataurl, col_names = F)
good_cols <- c("class",
"alcohol",
'malic_acid',
'ash',
'alkalinity',
'magnesium',
'total_phenols',
'flavanoids',
'nonflavonoids_phenols',
'proanthocyanins',
'color_intensity',
'hue',
'dilution',
'proline'
)
fix_cols <- function(df){
colnames(df) <- good_cols
df$class <- as.factor(df$class)
df
}
good_cols <- c("class",
"alcohol",
'malic_acid',
'ash',
'alkalinity',
'magnesium',
'total_phenols',
'flavanoids',
'nonflavonoids_phenols',
'proanthocyanins',
'color_intensity',
'hue',
'dilution',
'proline'
)
fix_cols <- function(df){
colnames(df) <- good_cols
df$class <- as.factor(df$class)
df
}
wine <- fix_cols(wine)
wine
set.seed(12345)
## WARNING: Danger function
split <- function(df, p = 0.75, list = FALSE, ...) {
train_ind <- createDataPartition(df[[1]], p = p, list = list)
cat("creating training dataset...\n")
training <<- df[train_ind, ]
cat("completed training dataset, creating test set\n")
test <<- df[-train_ind, ]
cat("done")
}
split(wine)
library(doParallel)
numcores <- parallel::detectCores() -1
cl <- makePSOCKcluster(numcores)
registerDoParallel(cl)
set.seed(655)
trainMethod <- trainControl(method = "repeatedcv",
number = 10,
repeats = 3)
knn_fit <- train(class ~ .,
data = training,
method = "knn",
trControl = trainMethod,
preProcess = c("center", "scale"),
tuneLength = 10)
library(pander)
pander(knn_fit)
set.seed(655)
trainMethod <- trainControl(method = "repeatedcv",
number = 10,
repeats = 3)
knn_fit <- train(class ~ .,
data = training,
method = "knn",
trControl = trainMethod,
preProcess = c("center", "scale"),
tuneLength = 10)
library(pander)
knn_fit
str(knn_fit)
summary(knn_fit)
(knn_fit)[1]
(knn_fit)[2]
knn_fit
plot(knn_fit)
set.seed(3033)
set.seed(3033)
## WARNING: Danger function
split <- function(df, p = 0.75, list = FALSE, ...) {
train_ind <- createDataPartition(df[[1]], p = p, list = list)
cat("creating training dataset...\n")
training <<- df[train_ind, ]
cat("completed training dataset, creating test set\n")
test <<- df[-train_ind, ]
cat("done")
}
split(wine)
set.seed(3333)
trainMethod <- trainControl(method = "repeatedcv",
number = 10,
repeats = 3)
knn_fit <- train(class ~ .,
data = training,
method = "knn",
trControl = trainMethod,
preProcess = c("center", "scale"),
tuneLength = 10)
library(pander)
knn_fit
plot(knn_fit)
test_pred <- predict(knn_fit, newdata = test)
test_pred <- predict(knn_fit, newdata = test)
test_pred
knn_fit2 <- knn3(training, training$class, k = 11)
knn_fit2 <- knn3(training, training$class, k = 11)
knn_fit2
knn_fit2 <- knn3(training, training$class, k = 15)
knn_fit2
cm <- confusionMatrix(test_pred, test$class)
cm
modelmetrics <- function(predicted, truth){
tab <- table(predicted, truth)
list(
precision(tab),
recall(tab),
F_meas(tab)
)
}
modelmetrics(test_pred, test$class)
f1_score <- function(predicted, expected, positive.class="1") {
cm = confusionMatrix(predicted, expected)
precision <- diag(cm) / colSums(cm)
recall <- diag(cm) / rowSums(cm)
f1 <-  ifelse(precision + recall == 0, 0, 2 * precision * recall / (precision + recall))
#Assuming that F1 is zero when it's not possible compute it
f1[is.na(f1)] <- 0
#Binary F1 or Multi-class macro-averaged F1
ifelse(nlevels(expected) == 2, f1[positive.class], mean(f1))
}
accuracy <- function(predicted, expected){
cm <- confusionMatrix(predicted, expected)
sum(diag(cm)/length(test$class))
}
f1_score(test_pred, test$class)
f1_score <- function(predicted, expected, positive.class="1") {
cm = confusionMatrix(predicted, expected)[[1]]
precision <- diag(cm) / colSums(cm)
recall <- diag(cm) / rowSums(cm)
f1 <-  ifelse(precision + recall == 0, 0, 2 * precision * recall / (precision + recall))
#Assuming that F1 is zero when it's not possible compute it
f1[is.na(f1)] <- 0
#Binary F1 or Multi-class macro-averaged F1
ifelse(nlevels(expected) == 2, f1[positive.class], mean(f1))
}
accuracy <- function(predicted, expected){
cm <- confusionMatrix(predicted, expected)
sum(diag(cm)/length(test$class))
}
f1_score(test_pred, test$class)
str(cm)
conmat <- function(predicted, expected){
cm <- as.matrix(table(Actual = as.factor(expected$class), Predicted = predicted))
cm
}
f1_score <- function(predicted, expected, positive.class="1") {
conmat = confusionMatrix(predicted, expected)
precision <- diag(cm) / colSums(cm)
recall <- diag(cm) / rowSums(cm)
f1 <-  ifelse(precision + recall == 0, 0, 2 * precision * recall / (precision + recall))
#Assuming that F1 is zero when it's not possible compute it
f1[is.na(f1)] <- 0
#Binary F1 or Multi-class macro-averaged F1
ifelse(nlevels(expected) == 2, f1[positive.class], mean(f1))
}
accuracy <- function(predicted, expected){
cm <- confusionMatrix(predicted, expected)
sum(diag(cm)/length(test$class))
}
f1_score(test_pred, test$class)
confusionMatrix(test_pred, test$class)
confusionMatrix(test_pred, test$class)
```{r}
conmat <- function(predicted, expected){
cm <- as.matrix(table(Actual = as.factor(expected$class), Predicted = predicted))
cm
}
f1_score <- function(predicted, expected, positive.class="1") {
cm = confusionMatrix(predicted, expected)
precision <- diag(cm) / colSums(cm)
recall <- diag(cm) / rowSums(cm)
f1 <-  ifelse(precision + recall == 0, 0, 2 * precision * recall / (precision + recall))
#Assuming that F1 is zero when it's not possible compute it
f1[is.na(f1)] <- 0
#Binary F1 or Multi-class macro-averaged F1
ifelse(nlevels(expected) == 2, f1[positive.class], mean(f1))
}
accuracy <- function(predicted, expected){
cm <- confusionMatrix(predicted, expected)
sum(diag(cm)/length(test$class))
}
f1_score(test_pred, test$class)
View(cm)
rm(cm)
f1_score(test_pred, test$class)
conmat <- function(predicted, expected){
cm <- as.matrix(table(Actual = as.factor(expected$class), Predicted = predicted))
cm
}
f1_score <- function(predicted, expected, positive.class="1") {
cm = conmat(predicted, expected)
precision <- diag(cm) / colSums(cm)
recall <- diag(cm) / rowSums(cm)
f1 <-  ifelse(precision + recall == 0, 0, 2 * precision * recall / (precision + recall))
#Assuming that F1 is zero when it's not possible compute it
f1[is.na(f1)] <- 0
#Binary F1 or Multi-class macro-averaged F1
ifelse(nlevels(expected) == 2, f1[positive.class], mean(f1))
}
accuracy <- function(predicted, expected){
cm <- confusionMatrix(predicted, expected)
sum(diag(cm)/length(test$class))
}
cm <- conmat(predicted, expected)
accuracy <- function(predicted, expected){
cm <- conmat(predicted, expected)
sum(diag(cm)/length(test$class))
}
f1_score(test_pred, test$class)
f1_score(test_pred, test$class)
f1_score(test_pred, test)
get_scores <- function(predictions, test){
f1 <- f1_score(predictions,test)
acc <- accuracy(predictions,test)
scores <- c(accuracy = acc, f1 = f1)
scores
}
pander(get_scores(test_pred, test))
conmat <- function(predicted, expected){
cm <- as.matrix(table(Actual = as.factor(expected$class), Predicted = predicted))
cm
}
f1_score <- function(predicted, expected, positive.class="1") {
cm = conmat(predicted, expected)
precision <- diag(cm) / colSums(cm)
recall <- diag(cm) / rowSums(cm)
f1 <-  ifelse(precision + recall == 0, 0, 2 * precision * recall / (precision + recall))
#Assuming that F1 is zero when it's not possible compute it
f1[is.na(f1)] <- 0
#Binary F1 or Multi-class macro-averaged F1
ifelse(nlevels(expected) == 2, f1[positive.class], mean(f1))
}
accuracy <- function(predicted, expected){
cm <- conmat(predicted, expected)
sum(diag(cm)/length(test$class))
}
get_scores <- function(predictions, test){
f1 <- f1_score(predictions,test)
acc <- accuracy(predictions,test)
scores <- c(accuracy = acc, f1 = f1)
scores
}
pander(get_scores(test_pred, test))
nb_fit <- train(training,
training$class,
trControl = trainMethod,
method = "nb",
tuneLength = 10
)
nb_fit <- train(training,
training$class,
trControl = trainMethod,
method = "nb",
tuneLength = 10
)
```
nb_fit <- train(training,
training$class,
trControl = trainMethod,
method = "nb",
tuneLength = 10
)
nb_fit
plot(nb_fit)
nb_pred <- predict(nb_fit, test)
nb_pred <- predict(nb_fit, newdata = test)
nb_pred
confusionMatrix(nb_pred, test$class)
get_scores(nb_pred, test)
fastNaiveBayes.detect_distribution(x, nrows = nrow(x))
y <- train$class
x <- train[-1]
y <- trainng$class
x <- training[-1]
y <- training$class
x <- training[-1]
fastNaiveBayes.detect_distribution(x, nrows = nrow(x))
dist <- fastNaiveBayes.detect_distribution(x, nrows = nrow(x))
dist
fast_nb_fit <- fastNaiveBayes.mixed(x,y)
fast_nb_fit
summary(fast_nb_fit)
plot(fast_nb_fit)
fast_pred <- predict(fast_nb_fit, test[-1])
fast_pred
confusionMatrix(fast_pred, test$class)
get_scores(fast_pred, test)
confusionMatrix(fast_pred, test$class)
get_scores(fast_pred, test)
